# å¼‚æ­¥æ–‡ä»¶å¤„ç†ç³»ç»Ÿè®¾ç½®æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨æœåŠ¡

é¦–å…ˆç¡®ä¿ä»¥ä¸‹æœåŠ¡å·²å¯åŠ¨ï¼š

#### PostgreSQL æ•°æ®åº“
```bash
# å¦‚æœä½¿ç”¨Docker
docker run --name postgres-aicg \
  -e POSTGRES_DB=aicg_platform \
  -e POSTGRES_USER=aicg_user \
  -e POSTGRES_PASSWORD=aicg_password \
  -p 5432:5432 \
  -d postgres:15

# æˆ–ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„PostgreSQL
sudo systemctl start postgresql
```

#### Redis æ¶ˆæ¯é˜Ÿåˆ—
```bash
# å¦‚æœä½¿ç”¨Docker
docker run --name redis-aicg -p 6379:6379 -d redis:7-alpine

# æˆ–ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„Redis
redis-server
```

#### MinIO å¯¹è±¡å­˜å‚¨
```bash
# å¦‚æœä½¿ç”¨Docker
docker run --name minio-aicg \
  -p 9000:9000 -p 9001:9001 \
  -e MINIO_ROOT_USER=minioadmin \
  -e MINIO_ROOT_PASSWORD=minioadmin \
  -v /tmp/minio-data:/data \
  minio/minio server /data --console-address ":9001"
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

åˆ›å»º `.env` æ–‡ä»¶ï¼š
```bash
# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql+asyncpg://aicg_user:aicg_password@localhost:5432/aicg_platform

# Redisé…ç½®
REDIS_URL=redis://localhost:6379/0

# Celeryé…ç½®
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# MinIOé…ç½®
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_SECURE=false

# åº”ç”¨é…ç½®
DEBUG=true
SECRET_KEY=your-secret-key-here
```

### 3. åˆå§‹åŒ–æ•°æ®åº“

```bash
cd backend

# è¿è¡Œæ•°æ®åº“è¿ç§»
alembic upgrade head

# æˆ–åœ¨å¼€å‘ç¯å¢ƒåˆ›å»ºè¡¨
python -c "
import asyncio
from src.core.database import initialize_database
asyncio.run(initialize_database())
"
```

### 4. å¯åŠ¨Celery Worker

```bash
# å¯åŠ¨Celery Workerï¼ˆæ–‡ä»¶å¤„ç†ï¼‰
# æ³¨æ„ï¼šCelery workerå¯åŠ¨æ—¶ä¼šè‡ªåŠ¨åˆå§‹åŒ–æ•°æ®åº“å¼•æ“
celery -A src.tasks.file_processing worker --loglevel=info --concurrency=4

# å¯åŠ¨Celery Beatï¼ˆå®šæ—¶ä»»åŠ¡ï¼Œå¯é€‰ï¼‰
celery -A src.tasks.file_processing beat --loglevel=info
```

**é‡è¦æç¤º**ï¼š
- Celery workerå¯åŠ¨æ—¶ä¼šè‡ªåŠ¨åˆå§‹åŒ–æ•°æ®åº“å¼•æ“
- å¦‚æœçœ‹åˆ°"æ•°æ®åº“å¼•æ“åˆå§‹åŒ–å¤±è´¥"é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“é…ç½®
- æ¯ä¸ªworkerè¿›ç¨‹å…±äº«åŒä¸€ä¸ªæ•°æ®åº“å¼•æ“ï¼Œæé«˜æ€§èƒ½

### 5. å¯åŠ¨APIæœåŠ¡

```bash
# å¼€å‘æ¨¡å¼
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000

# æˆ–ç”Ÿäº§æ¨¡å¼
uvicorn src.main:app --host 0.0.0.0 --port 8000 --workers 4
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åˆ›å»ºé¡¹ç›®å¹¶æäº¤æ–‡ä»¶å¤„ç†ä»»åŠ¡

```python
import requests
import json

# 1. åˆ›å»ºé¡¹ç›®
project_data = {
    "title": "æˆ‘çš„æµ‹è¯•é¡¹ç›®",
    "description": "æµ‹è¯•å¼‚æ­¥æ–‡ä»¶å¤„ç†åŠŸèƒ½",
    "file_name": "test_document.txt"
}

response = requests.post(
    "http://localhost:8000/api/v1/projects/",
    json=project_data,
    headers={"Authorization": "Bearer your-jwt-token"}
)

project = response.json()
project_id = project["id"]

# 2. ä¸Šä¼ æ–‡ä»¶å¹¶å¯åŠ¨å¤„ç†
files = {"file": open("test_document.txt", "rb")}
response = requests.post(
    f"http://localhost:8000/api/v1/projects/{project_id}/upload/",
    files=files,
    headers={"Authorization": "Bearer your-jwt-token"}
}

upload_result = response.json()
print(f"ä»»åŠ¡ID: {upload_result['task_id']}")
```

### æŸ¥è¯¢å¤„ç†çŠ¶æ€

```python
# æŸ¥è¯¢å¤„ç†çŠ¶æ€
response = requests.get(
    f"http://localhost:8000/api/v1/projects/{project_id}/status/",
    headers={"Authorization": "Bearer your-jwt-token"}
)

status = response.json()
print(f"å¤„ç†çŠ¶æ€: {status['status']}")
print(f"è¿›åº¦: {status['processing_progress']}%")
print(f"ç« èŠ‚æ•°: {status['chapters_count']}")
print(f"æ®µè½æ•°: {status['paragraphs_count']}")
print(f"å¥å­æ•°: {status['sentences_count']}")
```

### é‡è¯•å¤±è´¥çš„ä»»åŠ¡

```python
# å¦‚æœä»»åŠ¡å¤±è´¥ï¼Œå¯ä»¥é‡è¯•
response = requests.post(
    f"http://localhost:8000/api/v1/projects/{project_id}/retry/",
    headers={"Authorization": "Bearer your-jwt-token"}
)

retry_result = response.json()
print(f"é‡è¯•ä»»åŠ¡ID: {retry_result['task_id']}")
```

## ğŸ”§ å¼€å‘å’Œè°ƒè¯•

### æŸ¥çœ‹Celeryä»»åŠ¡çŠ¶æ€

```bash
# æŸ¥çœ‹æ´»è·ƒä»»åŠ¡
celery -A src.tasks.file_processing inspect active

# æŸ¥çœ‹å·²æ³¨å†Œä»»åŠ¡
celery -A src.tasks.file_processing inspect registered

# æŸ¥çœ‹ä»»åŠ¡ç»Ÿè®¡
celery -A src.tasks.file_processing inspect stats
```

### ç›‘æ§æ—¥å¿—

```bash
# æŸ¥çœ‹Workeræ—¥å¿—
tail -f celery_worker.log

# æˆ–å®æ—¶æŸ¥çœ‹
celery -A src.tasks.file_processing worker --loglevel=debug
```

### æµ‹è¯•å•ä¸ªä»»åŠ¡

```python
from src.tasks.file_processing import process_uploaded_file

# ç›´æ¥è°ƒç”¨ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
result = process_uploaded_file.apply(
    args=["project-123", "user-456"],
    kwargs={"file_path": "/path/to/file.txt"}
).get()

print(f"å¤„ç†ç»“æœ: {result}")
```

### å¥åº·æ£€æŸ¥

```python
from src.tasks.file_processing import health_check

# æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€
health = health_check.apply().get()
print(f"ç³»ç»ŸçŠ¶æ€: {health}")
```

## âš ï¸ å¸¸è§é—®é¢˜

### 1. æ•°æ®åº“è¿æ¥å¤±è´¥
```bash
# æ£€æŸ¥æ•°æ®åº“é…ç½®
echo $DATABASE_URL

# æµ‹è¯•æ•°æ®åº“è¿æ¥
python -c "
import asyncio
from src.core.database import test_database_connection
print('æ•°æ®åº“è¿æ¥:', asyncio.run(test_database_connection()))
"
```

### 2. Redisè¿æ¥å¤±è´¥
```bash
# æ£€æŸ¥RedisçŠ¶æ€
redis-cli ping

# æ£€æŸ¥Celeryè¿æ¥
celery -A src.tasks.file_processing inspect ping
```

### 3. ä»»åŠ¡æ‰§è¡Œå¤±è´¥
```bash
# æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯
celery -A src.tasks.file_processing worker --loglevel=error

# æ£€æŸ¥ä»»åŠ¡é…ç½®
celery -A src.tasks.file_processing conf
```

### 4. å†…å­˜ä½¿ç”¨è¿‡é«˜
```bash
# é™ä½Workerå¹¶å‘æ•°
celery -A src.tasks.file_processing worker --concurrency=2 --loglevel=info

# è®¾ç½®ä»»åŠ¡è¶…æ—¶
celery -A src.tasks.file_processing worker --task-time-limit=300 --loglevel=info
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æ•°æ®åº“è¿æ¥æ± é…ç½®

åœ¨ `src/core/config.py` ä¸­è°ƒæ•´ï¼š
```python
# å¢åŠ è¿æ¥æ± å¤§å°
DATABASE_POOL_SIZE = 20
DATABASE_MAX_OVERFLOW = 30
```

### 2. Celery Workeré…ç½®

```bash
# å¢åŠ Workerè¿›ç¨‹æ•°
celery -A src.tasks.file_processing worker --concurrency=8

# è®¾ç½®é¢„å–æ•°é‡
celery -A src.tasks.file_processing worker --prefetch-multiplier=1
```

### 3. ä»»åŠ¡ä¼˜å…ˆçº§

```python
from celery import current_app

@current_app.task(bind=True, name='high_priority_process', priority=9)
def high_priority_process(self, project_id: str):
    # é«˜ä¼˜å…ˆçº§ä»»åŠ¡
    pass
```

## ğŸ” ç›‘æ§å’Œç»´æŠ¤

### 1. ä½¿ç”¨Flowerç›‘æ§Celery

```bash
# å®‰è£…Flower
pip install flower

# å¯åŠ¨Flower
celery -A src.tasks.file_processing flower --port=5555

# è®¿é—®ç›‘æ§ç•Œé¢
open http://localhost:5555
```

### 2. æ•°æ®åº“æ€§èƒ½ç›‘æ§

```python
from src.core.database import get_database_stats

stats = asyncio.run(get_database_stats())
print(f"æ•°æ®åº“ç»Ÿè®¡: {stats}")
```

### 3. æ¸…ç†è¿‡æœŸä»»åŠ¡

```bash
# æ¸…ç†è¿‡æœŸä»»åŠ¡ç»“æœ
celery -A src.tasks.file_processing purge

# è®¾ç½®ä»»åŠ¡ç»“æœè¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
celery -A src.tasks.file_processing worker --task-result-expires=3600
```

## ğŸ“š APIæ–‡æ¡£

å¯åŠ¨APIæœåŠ¡åï¼Œå¯ä»¥è®¿é—®ï¼š
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## ğŸ†˜ æ•…éšœæ’é™¤

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ’æŸ¥ï¼š

1. **æ£€æŸ¥æœåŠ¡çŠ¶æ€**ï¼šç¡®ä¿æ‰€æœ‰ä¾èµ–æœåŠ¡è¿è¡Œæ­£å¸¸
2. **æŸ¥çœ‹æ—¥å¿—**ï¼šæ£€æŸ¥åº”ç”¨å’ŒCeleryæ—¥å¿—
3. **éªŒè¯é…ç½®**ï¼šç¡®è®¤ç¯å¢ƒå˜é‡é…ç½®æ­£ç¡®
4. **æµ‹è¯•è¿æ¥**ï¼šä½¿ç”¨å¥åº·æ£€æŸ¥å·¥å…·æµ‹è¯•å„ç»„ä»¶
5. **æŸ¥çœ‹ä»»åŠ¡**ï¼šä½¿ç”¨Celery inspectå‘½ä»¤æ£€æŸ¥ä»»åŠ¡çŠ¶æ€

éœ€è¦æ›´å¤šå¸®åŠ©ï¼Œè¯·æŸ¥çœ‹é¡¹ç›®æ–‡æ¡£æˆ–æäº¤Issueã€‚