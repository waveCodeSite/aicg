"""
æµ‹è¯•è§†é¢‘æ‹¼æ¥åŠŸèƒ½ - ç”¨äºæµ‹è¯•å»é™¤é‡å¤å¸§çš„è§†é¢‘æ‹¼æ¥

ä½¿ç”¨æ–¹æ³•:
python scripts/test_video_concatenation.py --chapter-id <ç« èŠ‚ID>
python scripts/test_video_concatenation.py --chapter-id <ç« èŠ‚ID> --no-remove-duplicates  # å¯¹æ¯”æµ‹è¯•
python scripts/test_video_concatenation.py --video-dir <è§†é¢‘ç›®å½•>  # ä½¿ç”¨æœ¬åœ°è§†é¢‘æµ‹è¯•
"""

import asyncio
import sys
from pathlib import Path
import argparse
import tempfile
import shutil

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from src.core.database import get_async_db
from src.core.logging import get_logger
from src.utils.ffmpeg_utils import concatenate_videos, check_ffmpeg_installed
from sqlalchemy import select
from sqlalchemy.orm import joinedload

logger = get_logger(__name__)


async def test_concatenation_from_chapter(chapter_id: str, remove_duplicates: bool = True):
    """
    ä»ç« èŠ‚çš„è¿‡æ¸¡è§†é¢‘æµ‹è¯•æ‹¼æ¥åŠŸèƒ½
    
    Args:
        chapter_id: ç« èŠ‚ID
        remove_duplicates: æ˜¯å¦å»é™¤é‡å¤å¸§
    """
    temp_dir = None
    
    try:
        # æ£€æŸ¥FFmpeg
        if not check_ffmpeg_installed():
            logger.error("FFmpegæœªå®‰è£…æˆ–ä¸å¯ç”¨")
            return
        
        # è·å–æ•°æ®åº“ä¼šè¯
        async with get_async_db() as db_session:
            # æŸ¥è¯¢ç« èŠ‚çš„æ‰€æœ‰è¿‡æ¸¡è§†é¢‘
            from src.models.movie import MovieScript, MovieShotTransition
            
            # å…ˆè·å–å‰§æœ¬ID
            result = await db_session.execute(
                select(MovieScript)
                .where(MovieScript.chapter_id == chapter_id)
            )
            script = result.scalar_one_or_none()
            
            if not script:
                logger.error(f"ç« èŠ‚æ²¡æœ‰ç”µå½±å‰§æœ¬: {chapter_id}")
                return
            
            # è·å–æ‰€æœ‰å·²å®Œæˆçš„è¿‡æ¸¡è§†é¢‘
            result = await db_session.execute(
                select(MovieShotTransition)
                .where(MovieShotTransition.script_id == script.id)
                .where(MovieShotTransition.video_url.isnot(None))
                .where(MovieShotTransition.status == 'completed')
                .order_by(MovieShotTransition.order_index).limit(6)
            )
            transitions = result.scalars().all()
            
            if len(transitions) < 2:
                logger.error(f"è¿‡æ¸¡è§†é¢‘æ•°é‡ä¸è¶³(éœ€è¦è‡³å°‘2ä¸ª): {len(transitions)}")
                return
            
            logger.info(f"æ‰¾åˆ° {len(transitions)} ä¸ªè¿‡æ¸¡è§†é¢‘")
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = Path(tempfile.mkdtemp(prefix="test_concat_"))
            logger.info(f"ä¸´æ—¶ç›®å½•: {temp_dir}")
            
            # ä¸‹è½½è¿‡æ¸¡è§†é¢‘
            from src.utils.storage import get_storage_client
            storage = await get_storage_client()
            
            video_paths = []
            for idx, transition in enumerate(transitions):
                logger.info(f"ğŸ“¥ ä¸‹è½½è¿‡æ¸¡è§†é¢‘ {idx + 1}/{len(transitions)}: {transition.video_url}")
                
                video_path = temp_dir / f"transition_{idx:03d}.mp4"
                content = await storage.download_file(transition.video_url)
                
                with open(video_path, 'wb') as f:
                    f.write(content)
                
                video_paths.append(video_path)
                logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {len(content)} bytes")
            
            # æ‰§è¡Œæ‹¼æ¥æµ‹è¯•
            await _test_concatenation(video_paths, temp_dir, remove_duplicates, trim_frames=40)
            
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if temp_dir and temp_dir.exists():
            try:
                shutil.rmtree(temp_dir)
                logger.info(f"æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
            except Exception as e:
                logger.error(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")


async def _test_concatenation(video_paths: list, temp_dir: Path, remove_duplicates: bool, trim_frames: int = 3):
    """
    æ‰§è¡Œæ‹¼æ¥æµ‹è¯•
    
    Args:
        video_paths: è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        temp_dir: ä¸´æ—¶ç›®å½•
        remove_duplicates: æ˜¯å¦å»é™¤é‡å¤å¸§
        trim_frames: æ¯å¤„è£å‰ªçš„å¸§æ•°
    """
    import time
    
    # å‡†å¤‡è¾“å‡º
    output_dir = Path("./test_output")
    output_dir.mkdir(exist_ok=True)
    
    suffix = f"_trim{trim_frames}" if remove_duplicates else "_with_dup"
    output_file = output_dir / f"concatenated{suffix}.mp4"
    concat_file = temp_dir / "concat.txt"
    
    logger.info("=" * 60)
    logger.info(f"å¼€å§‹æ‹¼æ¥æµ‹è¯•...")
    logger.info(f"è§†é¢‘æ•°é‡: {len(video_paths)}")
    logger.info(f"å»é™¤é‡å¤å¸§: {'æ˜¯' if remove_duplicates else 'å¦'}")
    if remove_duplicates:
        logger.info(f"è£å‰ªå¸§æ•°: {trim_frames}å¸§/å¤„")
    logger.info("=" * 60)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # æ‰§è¡Œæ‹¼æ¥
    success = concatenate_videos(
        video_paths,
        output_file,
        concat_file,
        remove_duplicate_frames=remove_duplicates,
        trim_frames=trim_frames
    )
    
    # è®°å½•ç»“æŸæ—¶é—´
    elapsed_time = time.time() - start_time
    
    if success:
        logger.info("=" * 60)
        logger.info(f"âœ… è§†é¢‘æ‹¼æ¥æˆåŠŸ!")
        logger.info(f"ğŸ“¹ è¾“å‡ºæ–‡ä»¶: {output_file.absolute()}")
        logger.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {output_file.stat().st_size / 1024 / 1024:.2f} MB")
        logger.info(f"â±ï¸  å¤„ç†æ—¶é—´: {elapsed_time:.2f} ç§’")
        logger.info(f"ğŸ”§ å»é™¤é‡å¤å¸§: {'æ˜¯' if remove_duplicates else 'å¦'}")
        logger.info("=" * 60)
        
        # æ˜¾ç¤ºè§†é¢‘ä¿¡æ¯
        from src.utils.ffmpeg_utils import get_audio_duration
        duration = get_audio_duration(str(output_file))
        if duration:
            logger.info(f"ğŸ¬ è§†é¢‘æ—¶é•¿: {duration:.2f} ç§’")
        
        # æç¤ºå¯¹æ¯”æµ‹è¯•
        if remove_duplicates:
            logger.info("")
            logger.info("ğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ --no-remove-duplicates å‚æ•°ç”Ÿæˆå¯¹æ¯”è§†é¢‘")
        else:
            logger.info("")
            logger.info("ğŸ’¡ æç¤º: å¯ä»¥å»æ‰ --no-remove-duplicates å‚æ•°æµ‹è¯•å»é‡æ•ˆæœ")
            
    else:
        logger.error("=" * 60)
        logger.error(f"âŒ è§†é¢‘æ‹¼æ¥å¤±è´¥!")
        logger.error("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æµ‹è¯•è§†é¢‘æ‹¼æ¥åŠŸèƒ½(å»é™¤é‡å¤å¸§)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä»ç« èŠ‚çš„è¿‡æ¸¡è§†é¢‘æµ‹è¯•
  python scripts/test_video_concatenation.py --chapter-id abc123...
  
  # å¯¹æ¯”æµ‹è¯•(ä¸å»é™¤é‡å¤å¸§)
  python scripts/test_video_concatenation.py --chapter-id abc123... --no-remove-duplicates
  
  # ä½¿ç”¨æœ¬åœ°è§†é¢‘ç›®å½•æµ‹è¯•
  python scripts/test_video_concatenation.py --video-dir ./test_videos
        """
    )
    
    # äº’æ–¥å‚æ•°ç»„
    source_group = parser.add_mutually_exclusive_group(required=True)
    source_group.add_argument(
        "--chapter-id",
        help="ç« èŠ‚ID (UUIDæ ¼å¼)"
    )
    
    parser.add_argument(
        "--no-remove-duplicates",
        action="store_true",
        help="ä¸å»é™¤é‡å¤å¸§(ç”¨äºå¯¹æ¯”æµ‹è¯•)"
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šæ˜¯å¦å»é™¤é‡å¤å¸§
    remove_duplicates = not args.no_remove_duplicates
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_concatenation_from_chapter(args.chapter_id, remove_duplicates))


if __name__ == "__main__":
    main()
